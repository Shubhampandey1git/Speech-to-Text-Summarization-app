{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b8252c",
   "metadata": {},
   "source": [
    "# **Data Cleaning and Preprocessing For Hindi/English ASR (LibriSpeech ASR/ IndicSpeech)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81a059",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892549a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Final Year Project\\Speech-to-Text Summarization System for Smart Note-Taking\\Virtual\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7143776",
   "metadata": {},
   "source": [
    "### **Loading the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba587d58",
   "metadata": {},
   "source": [
    "***Making Dir***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ef4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean/asr/english_audio\", exist_ok=True)\n",
    "os.makedirs(\"../data/clean/asr/hindi_audio\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a005cfc",
   "metadata": {},
   "source": [
    "***Loading datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3be053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully with raw file paths\n",
      "                                          audio_path  \\\n",
      "0  /home/albert/.cache/huggingface/datasets/downl...   \n",
      "1  /home/albert/.cache/huggingface/datasets/downl...   \n",
      "2  /home/albert/.cache/huggingface/datasets/downl...   \n",
      "3  /home/albert/.cache/huggingface/datasets/downl...   \n",
      "4  /home/albert/.cache/huggingface/datasets/downl...   \n",
      "\n",
      "                                                text  \n",
      "0  CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE B...  \n",
      "1  MARGUERITE TO BE UNABLE TO LIVE APART FROM ME ...  \n",
      "2  I WISHED ABOVE ALL NOT TO LEAVE MYSELF TIME TO...  \n",
      "3  ASSUMED ALL AT ONCE AN APPEARANCE OF NOISE AND...  \n",
      "4  NOTHING IS SO EXPENSIVE AS THEIR CAPRICES FLOW...  \n"
     ]
    }
   ],
   "source": [
    "# English\n",
    "# Load dataset with \"keep_in_memory=True\" to make access faster\n",
    "dataset_en = load_dataset(\"librispeech_asr\", \"clean\", split=\"train.100[:1%]\", keep_in_memory=True)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe — keeps only useful columns\n",
    "df_en = dataset_en.to_pandas()[[\"file\", \"text\"]]\n",
    "df_en.rename(columns={\"file\": \"audio_path\"}, inplace=True)\n",
    "\n",
    "print(\"✅ Dataset loaded successfully with raw file paths\")\n",
    "print(df_en.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c2cec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'AI4Bharat/IndicSpeech' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Hindi\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset_hi \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAI4Bharat/IndicSpeech\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain[:1\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset_hi)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_hi[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32me:\\Final Year Project\\Speech-to-Text Summarization System for Smart Note-Taking\\Virtual\\lib\\site-packages\\datasets\\load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1389\u001b[0m )\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1393\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   1394\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   1395\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   1396\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   1397\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1398\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   1399\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1400\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1401\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1402\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1403\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1405\u001b[0m )\n\u001b[0;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32me:\\Final Year Project\\Speech-to-Text Summarization System for Smart Note-Taking\\Virtual\\lib\\site-packages\\datasets\\load.py:1132\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[1;32m-> 1132\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32me:\\Final Year Project\\Speech-to-Text Summarization System for Smart Note-Taking\\Virtual\\lib\\site-packages\\datasets\\load.py:1025\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1028\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1029\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1030\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Final Year Project\\Speech-to-Text Summarization System for Smart Note-Taking\\Virtual\\lib\\site-packages\\datasets\\load.py:980\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    978\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[0;32m    983\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m    984\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    987\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[0;32m    988\u001b[0m     )\n",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m: Dataset 'AI4Bharat/IndicSpeech' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "# Hindi\n",
    "dataset_hi = load_dataset(\"AI4Bharat/IndicSpeech\", \"hi\", split=\"train[:1%]\")\n",
    "print(dataset_hi)\n",
    "print(\"Sample:\", dataset_hi[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0837bc4",
   "metadata": {},
   "source": [
    "### **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc180027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "def clean_english_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "dataset_en = dataset_en.map(lambda x: {\"clean_text\": clean_english_text(x[\"text\"])})\n",
    "print(\"Cleaned sample:\", dataset_en[0][\"clean_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hindi\n",
    "import unicodedata\n",
    "\n",
    "def clean_hindi_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = re.sub(r\"[^\\u0900-\\u097Fa-zA-Z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "dataset_hi = dataset_hi.map(lambda x: {\"clean_text\": clean_hindi_text(x[\"sentence\"])})\n",
    "print(\"Cleaned Hindi sample:\", dataset_hi[0][\"clean_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83015d2d",
   "metadata": {},
   "source": [
    "### **Resample and Save Audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "def process_audio_en(example):\n",
    "    audio_array, sr = librosa.load(example[\"audio\"][\"path\"], sr=16000)\n",
    "    new_path = os.path.join(\"data/clean/asr/english_audio\", os.path.basename(example[\"audio\"][\"path\"]).replace(\".flac\", \".wav\"))\n",
    "    sf.write(new_path, audio_array, 16000)\n",
    "    example[\"resampled_path\"] = new_path\n",
    "    return example\n",
    "\n",
    "print(\"Resampling English audio files...\")\n",
    "dataset_en = dataset_en.map(process_audio_en)\n",
    "\n",
    "df_en = pd.DataFrame({\n",
    "    \"audio_path\": [ex[\"resampled_path\"] for ex in dataset_en],\n",
    "    \"text\": [ex[\"clean_text\"] for ex in dataset_en]\n",
    "})\n",
    "df_en.to_csv(\"data/clean/asr/english_asr_clean.csv\", index=False)\n",
    "print(\"Saved English ASR dataset to data/clean/asr/english_asr_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd487c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hindi\n",
    "def process_audio_hi(example):\n",
    "    audio_array, sr = librosa.load(example[\"audio\"][\"path\"], sr=16000)\n",
    "    new_path = os.path.join(\"data/clean/asr/hindi_audio\", os.path.basename(example[\"audio\"][\"path\"]).replace(\".wav\", \"_16k.wav\"))\n",
    "    sf.write(new_path, audio_array, 16000)\n",
    "    example[\"resampled_path\"] = new_path\n",
    "    return example\n",
    "\n",
    "print(\"Resampling Hindi audio files...\")\n",
    "dataset_hi = dataset_hi.map(process_audio_hi)\n",
    "\n",
    "df_hi = pd.DataFrame({\n",
    "    \"audio_path\": [ex[\"resampled_path\"] for ex in dataset_hi],\n",
    "    \"text\": [ex[\"clean_text\"] for ex in dataset_hi]\n",
    "})\n",
    "df_hi.to_csv(\"data/clean/asr/hindi_asr_clean.csv\", index=False)\n",
    "print(\"Saved Hindi ASR dataset to data/clean/asr/hindi_asr_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543946e6",
   "metadata": {},
   "source": [
    "### **Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525bfb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"English ASR samples:\", len(df_en))\n",
    "print(\"Hindi ASR samples:\", len(df_hi))\n",
    "\n",
    "print(\"\\nSample English row:\")\n",
    "print(df_en.sample(1))\n",
    "\n",
    "print(\"\\nSample Hindi row:\")\n",
    "print(df_hi.sample(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
